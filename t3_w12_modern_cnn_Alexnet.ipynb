{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "t3_w12_modern_cnn_Alexnet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPKxUEnpAuywD/uX/tCUj1k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lahrie/PembelajaranMesin-Week-12/blob/main/t3_w12_modern_cnn_Alexnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "89NyrKz1ezzD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training batches of our network\n",
        "EPOCHS = 10\n",
        "# size of each batch\n",
        "BATCH_SIZE = 512\n",
        "\n",
        "DEVICE = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "print(torch.__version__)\n",
        "print(DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Cpxnzl1e3PW",
        "outputId": "e2dcd6f3-cc92-4178-ef8b-14a4fa601df0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.10.0+cu111\n",
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#connect to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvUCmTWffBYs",
        "outputId": "b94a6bbc-e6a2-4542-c122-84062ceb3a99"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_file_path = '/content/fashion-mnist_train.csv'\n",
        "test_file_path = '/content/fashion-mnist_test.csv'"
      ],
      "metadata": {
        "id": "-g6PU3RvfMeh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_csv = pd.read_csv(train_file_path)\n",
        "test_csv = pd.read_csv(test_file_path)\n",
        "\n",
        "\n",
        "print(train_csv.shape)\n",
        "print(test_csv.shape)"
      ],
      "metadata": {
        "id": "sl4I59UofPNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_csv.info())\n",
        "print(train_csv.head())"
      ],
      "metadata": {
        "id": "jSePysnpfaLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<class 'pandas.core.frame.DataFrame'> RangeIndex: 60000 entries, 0 to 59999 Columns: 785 entries, label to pixel784 dtypes: int64(785) memory usage: 359.3 MB None label pixel1 pixel2 pixel3 ... pixel781 pixel782 pixel783 pixel784 0 2 0 0 0 ... 0 0 0 0 1 9 0 0 0 ... 0 0 0 0 2 6 0 0 0 ... 0 0 0 0 3 0 0 0 0 ... 0 0 0 0 4 3 0 0 0 ... 0 0 0 0\n",
        "\n",
        "[5 rows x 785 columns] To build our own dataset, need to create a class that inherits from the Dataset. Besides, function get_item() & len() must be defined at least\n",
        "\n",
        "get_item() return the specified image and its label len() return the number of dataset"
      ],
      "metadata": {
        "id": "4JOS96_ufmsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionDataset(Dataset):\n",
        "    def __init__(self, data, transform=None):        \n",
        "        self.fashion_MNIST = list(data.values)\n",
        "        self.transform = transform\n",
        "        \n",
        "        label, image = [], []\n",
        "        \n",
        "        for i in self.fashion_MNIST:\n",
        "            label.append(i[0])\n",
        "            image.append(i[1:])\n",
        "        self.labels = np.asarray(label)\n",
        "        self.images = np.asarray(image).reshape(-1, 28, 28).astype('float32')\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        label = self.labels[idx]\n",
        "        image = self.images[idx]      \n",
        "        \n",
        "        if self.transform is not None:\n",
        "            # transfrom the numpy array to PIL image before the transform function\n",
        "            pil_image = Image.fromarray(np.uint8(image)) \n",
        "            image = self.transform(pil_image)\n",
        "            \n",
        "        return image, label"
      ],
      "metadata": {
        "id": "B9kswmZnfwTL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform Because the input size of AlexNet is 227∗227 , and the image size of Fashion-MNIST is 28∗28 , so we need to resize the image in the transform function\n",
        "\n",
        "Since transforms.Resize() only works to the PIL Image,we transform the numpy array to PIL Image above"
      ],
      "metadata": {
        "id": "wFJgc2p1f2Ia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AlexTransform = transforms.Compose([\n",
        "    transforms.Resize((227, 227)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])"
      ],
      "metadata": {
        "id": "oyiLs0PSf342"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DataLoader\n",
        "train_loader = DataLoader(\n",
        "    FashionDataset(train_csv, transform=AlexTransform), \n",
        "    batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    FashionDataset(test_csv, transform=AlexTransform), \n",
        "    batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "kGKkim3Vf6V0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function to show an image\n",
        "def matplotlib_imshow(img):\n",
        "    img = img.mean(dim=0)\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(npimg, cmap=\"Greys\")\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# creat grid of images\n",
        "img_grid = torchvision.utils.make_grid(images[0])\n",
        "\n",
        "# show images & labels\n",
        "matplotlib_imshow(img_grid)\n",
        "print(class_names[labels[0]])"
      ],
      "metadata": {
        "id": "FaVLrTd1f_b6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class fasion_mnist_alexnet(nn.Module):  \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=96, kernel_size=11, stride=4, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(96, 256, 5, 1, 2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(3, 2)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 384, 3, 1, 1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(384, 384, 3, 1, 1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(384, 256, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(3, 2)\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Linear(256 * 6 * 6, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "        self.fc3 = nn.Linear(4096, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.conv5(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "\n",
        "        out = F.relu(self.fc1(out))  # 256*6*6 -> 4096\n",
        "        out = F.dropout(out, 0.5)\n",
        "        out = F.relu(self.fc2(out))\n",
        "        out = F.dropout(out, 0.5)\n",
        "        out = self.fc3(out)\n",
        "        out = F.log_softmax(out, dim=1)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "Nf0oCk1PgA4x"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MODEL\n",
        "model = fasion_mnist_alexnet().to(DEVICE)\n",
        "criterion = F.nll_loss\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "8bDzWTf1gDrN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        target = target.type(torch.LongTensor)\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (batch_idx + 1) % 30 == 0:\n",
        "            print(\"Train Epoch:{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "metadata": {
        "id": "pyCHS96QgGIV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target, reduction='sum').item()\n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "        test_loss /= len(test_loader.dataset)  # loss之和除以data数量 -> mean\n",
        "        print(\"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
        "            test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))\n",
        "        print('='*50)"
      ],
      "metadata": {
        "id": "qVdBBRTDgIYp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, EPOCHS+1):\n",
        "    train(model, DEVICE, train_loader, optimizer, epoch)\n",
        "    test(model, DEVICE, test_loader)"
      ],
      "metadata": {
        "id": "6Uqse-YzgMHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Epoch:1 [14848/60000 (25%)] Loss: 1.000911 Train Epoch:1 [30208/60000 (50%)] Loss: 0.628270 Train Epoch:1 [45568/60000 (75%)] Loss: 0.512569\n",
        "\n",
        "Test set: Average loss: 0.4322, Accuracy: 8394/10000 (84%)\n",
        "\n",
        "================================================== Train Epoch:2 [14848/60000 (25%)] Loss: 0.347188 Train Epoch:2 [30208/60000 (50%)] Loss: 0.319300 Train Epoch:2 [45568/60000 (75%)] Loss: 0.304157\n",
        "\n",
        "Test set: Average loss: 0.3269, Accuracy: 8806/10000 (88%)\n",
        "\n",
        "================================================== Train Epoch:3 [14848/60000 (25%)] Loss: 0.304715 Train Epoch:3 [30208/60000 (50%)] Loss: 0.298482 Train Epoch:3 [45568/60000 (75%)] Loss: 0.390529\n",
        "\n",
        "Test set: Average loss: 0.2964, Accuracy: 8920/10000 (89%)\n",
        "\n",
        "================================================== Train Epoch:4 [14848/60000 (25%)] Loss: 0.270463 Train Epoch:4 [30208/60000 (50%)] Loss: 0.254679 Train Epoch:4 [45568/60000 (75%)] Loss: 0.272952\n",
        "\n",
        "Test set: Average loss: 0.3941, Accuracy: 8464/10000 (85%)\n",
        "\n",
        "================================================== Train Epoch:5 [14848/60000 (25%)] Loss: 0.279809 Train Epoch:5 [30208/60000 (50%)] Loss: 0.237800 Train Epoch:5 [45568/60000 (75%)] Loss: 0.260069\n",
        "\n",
        "Test set: Average loss: 0.2362, Accuracy: 9122/10000 (91%)\n",
        "\n",
        "================================================== Train Epoch:6 [14848/60000 (25%)] Loss: 0.185056 Train Epoch:6 [30208/60000 (50%)] Loss: 0.205573 Train Epoch:6 [45568/60000 (75%)] Loss: 0.194789\n",
        "\n",
        "Test set: Average loss: 0.2280, Accuracy: 9141/10000 (91%)\n",
        "\n",
        "================================================== Train Epoch:7 [14848/60000 (25%)] Loss: 0.184588 Train Epoch:7 [30208/60000 (50%)] Loss: 0.221858 Train Epoch:7 [45568/60000 (75%)] Loss: 0.205771\n",
        "\n",
        "Test set: Average loss: 0.2280, Accuracy: 9172/10000 (92%)\n",
        "\n",
        "================================================== Train Epoch:8 [14848/60000 (25%)] Loss: 0.252704 Train Epoch:8 [30208/60000 (50%)] Loss: 0.201521 Train Epoch:8 [45568/60000 (75%)] Loss: 0.183978\n",
        "\n",
        "Test set: Average loss: 0.2088, Accuracy: 9218/10000 (92%)\n",
        "\n",
        "================================================== Train Epoch:9 [14848/60000 (25%)] Loss: 0.178087 Train Epoch:9 [30208/60000 (50%)] Loss: 0.201523 Train Epoch:9 [45568/60000 (75%)] Loss: 0.185059\n",
        "\n",
        "Test set: Average loss: 0.2255, Accuracy: 9182/10000 (92%)\n",
        "\n",
        "================================================== Train Epoch:10 [14848/60000 (25%)] Loss: 0.181494 Train Epoch:10 [30208/60000 (50%)] Loss: 0.162224 Train Epoch:10 [45568/60000 (75%)] Loss: 0.163105\n",
        "\n",
        "Test set: Average loss: 0.2345, Accuracy: 9175/10000 (92%)"
      ],
      "metadata": {
        "id": "qvhi2cqCgO64"
      }
    }
  ]
}